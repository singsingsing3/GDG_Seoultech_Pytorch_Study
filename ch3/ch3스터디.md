## 2. ğŸ”SSD
### âœï¸2.1 ë¬¼ì²´ ê°ì§€
- í•œ ì¥ì˜ ì‚¬ì§„ì— í¬í•¨ëœ ì—¬ëŸ¬ ë¬¼ì²´ì— ëŒ€í•´ ì˜ì—­ê³¼ ì´ë¦„ì„ í™•ì¸í•˜ëŠ” ì‘ì—…
- ì¶œë ¥
1. BBox(Bounding Box)ì˜ ìœ„ì¹˜ì™€ í¬ê¸° ì •ë³´
2. BBoxê°€ ì–´ë–¤ ë¬¼ì²´ì¸ì§€ ë‚˜íƒ€ë‚´ëŠ” label
3. confidence(ì‹ ë¢°ë„)
- ë¼ë²¨ ì •ë³´ëŠ” ê°ì§€í•˜ë ¤ëŠ” ë¬¼ì²´ì˜ í´ë˜ìŠ¤ ìˆ˜ + 1(ë°°ê²½ class)
### <SSD íë¦„>
- ì´ ì±…ì—ì„  VOC dataset í™œìš©
- SSD300 ì±„íƒ(ì´ë¯¸ì§€ í¬ê¸° 300x300)
- BBOXì˜ ì •ë³´ë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, DBox(default box, ì¼ë°˜ì  ì‚¬ê°í˜• box)ë¥¼ ì–´ë–»ê²Œ ë³€í˜•ì‹œì¼œì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¶œë ¥

1. 300x300 image resize

2. default box 8732ê°œ ì¤€ë¹„

3. SSDì— image ì…ë ¥

4. ì‹ ë¢°ë„ ë†’ì€ DBox ì¶”ì¶œ

5. offesetì •ë³´ë¡œ ìˆ˜ì • ë° ì¤‘ë³µ ì œê±°

6. ì¼ì • ì‹ ë¢°ë„ ì´ìƒì„ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ì„ ì •


### âœï¸2.2 DataSet êµ¬í˜„
- Annotation Data: ë¬¼ì²´ ìœ„ì¹˜ì™€ ë¼ë²¨ì„ ë‚˜íƒ€ë‚´ëŠ” BBox
-> **imgaeì™€ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì²˜ë¦¬í•´ì•¼í•œë‹¤**

#### ğŸ’¬ 1.íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ë³´ì
```
import os.path as osp
import random

import xml.etree.ElementTree as ET

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.utils.data as data



%matplotlib inline

torch.manual_seed(1234)
np.random.seed(1234)
random.seed(1234)
def make_datapath_list(rootpath):
  """
  ë°ì´í„° ê²½ë¡œë¥¼ ì €ì¥í•œ ë¦¬ìŠ¤íŠ¸ ì‘ì„±

  Parameters
  ----------
  rootpath:str
      ë°ì´í„° í´ë” ê²½ë¡œ

  Returns
  -------
  ret: train_img_list, train_anno_list, val_img_list, val_anno_list
      ë°ì´í„° ê²½ë¡œë¥¼ ì €ì¥í•œ ë¦¬ìŠ¤íŠ¸
  """
  # í™”ìƒ íŒŒì¼ê³¼ ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ì˜ ê²½ë¡œ í…œí”Œë¦¿ ì‘ì„±
  imagepath_template = osp.join(rootpath+'JPEGImages', '%s.jpg')
  annopath_template= osp.join(rootpath+'Annotations', '%s.xml')

  #í›ˆë ¨ ë° ê²€ì¦ íŒŒì¼ ID(íŒŒì¼ì´ë¦„) ì·¨ë“
  train_id_names=osp.join(rootpath+'ImageSets/Main/train.txt')
  val_id_names=osp.join(rootpath+'ImageSets/Main/val.txt')

  #í›ˆë ¨ ë°ì´í„°ì˜ í™”ìƒ íŒŒì¼ê³¼ ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ì˜ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì‘ì„±
  train_img_list=list()
  train_anno_list=list()

  for line in open(train_id_names):
    file_id=line.strip() #ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì œê±°
    img_path=(imagepath_template % file_id)
    anno_path=(annopath_template % file_id)
    train_img_list.append(img_path) # ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    train_anno_list.append(anno_path)

# ê²€ì¦ ë°ì´í„°ì˜ í™”ìƒíŒŒì¼ê³¼ ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ì˜ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì‘ì„±
  val_img_list=list()
  val_anno_list=list()

  for line in open(val_id_names):
    file_id=line.strip() #ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì œê±°
    img_path=(imagepath_template % file_id)
    anno_path=(annopath_template % file_id)
    val_img_list.append(img_path) # ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    val_anno_list.append(anno_path)

  return train_img_list, train_anno_list, val_img_list, val_anno_list
```
ë™ì‘ì´ ì˜ ë˜ëŠ”ì§€ ì°ì–´ë³´ì
```
# íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì‘ì„±
rootpath='/content/drive/MyDrive/Colab Notebooks/pytorch_advanced/2_objectdetection/data/VOCdevkit/VOC2012/'
train_img_list,train_anno_list,val_img_list,val_anno_list=make_datapath_list(rootpath)

#ë™ì‘ í™•ì¸
print(train_img_list[0])
```
> /content/drive/MyDrive/Colab Notebooks/pytorch_advanced/2_objectdetection/data/VOCdevkit/VOC2012/JPEGImages/2008_000008.jpg

ê²½ë¡œê°€ ì˜ ì¶œë ¥ ëœë‹¤

#### ğŸ’¬ 2.Annotation dataë¥¼ listë¡œ ë‹´ì•„ë‚´ì

ì´ì œ Annotation dataë¥¼ listë¡œ ë³€í™˜í•´ë³´ì
Anno dataëŠ” xmlíŒŒì¼ë¡œ ì£¼ì–´ì¡Œìœ¼ë©°
ì£¼ì–´ì§„ Anno Data ë‚´ë¶€ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.
```
<object>
		<name>person</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>174</xmin>
			<ymin>101</ymin>
			<xmax>349</xmax>
			<ymax>351</ymax>
		</bndbox>
		<part>
			<name>head</name>
			<bndbox>
				<xmin>169</xmin>
				<ymin>104</ymin>
				<xmax>209</xmax>
				<ymax>146</ymax>
			</bndbox>
		</part>
```
objectì— ëŒ€í•œ ì„¤ëª…ê³¼ BBoxì •ë³´ê°€ ë‹´ê²¨ìˆë‹¤
ì´ì œ ì´ xmlíŒŒì¼ì— ìˆëŠ” ì •ë³´ë¥¼ listì— ë‹´ì•„ë‚´ì
```
import xml.etree.ElementTree as ET
import numpy as np
# XML í˜•ì‹ì˜ ì–´ë…¸í…Œì´ì…˜ì„ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ”  í´ë˜ìŠ¤
class Anno_xml2list(object):
    """
    í•œ í™”ìƒì˜ XML í˜•ì‹ ì–´ë…¸í…Œì´ì…˜ ë°ì´í„°ë¥¼ í™”ìƒ í¬ê¸°ë¡œ normalizationí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜

    Attributes
    ---------
    classes: ë¦¬ìŠ¤íŠ¸
        VOCì˜ classëª…ì„ ì €ì¥í•œ ë¦¬ìŠ¤íŠ¸
    """
    def __init__(self,classes):
        self.classes=classes

    def __call__(self, xml_path, width, height):
        """
        í•œ í™”ìƒì˜ XML í˜•ì‹ ì–´ë…¸í…Œì´ì…˜ ë°ì´í„°ë¥¼ í™”ìƒ í¬ê¸°ë¡œ normalizationí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜

        Parameters
        ----------
        xml_path:str
            xml íŒŒì¼ ê²½ë¡œ
        width: int
            ëŒ€ìƒ í™”ìƒ í­
        height: int
            ëŒ€ìƒ í™”ìƒ ë†’ì´

        Returns
        ----------
        ret: [[xmin, ymin, xmax, ymax,label_ind], ...]
            ë¬¼ì²´ì˜ ì–´ë…¸í…Œì´ì…˜ ë°ì´í„°ë¥¼ ì €ì¥í•œ ë¦¬ìŠ¤íŠ¸. í™”ìƒì— ì¡´ì¬í•˜ëŠ” ë¬¼ì²´ ìˆ˜ë§Œí¼ ìš”ì†Œë¥¼ ê°€ì§„ë‹¤

        """

        # í™”ìƒ ë‚´ ëª¨ë“  ë¬¼ì²´ì˜ ì–´ë…¸í…Œì´ì…˜ì„ ì´ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        ret= []

        # xml íŒŒì¼ ë¡œë“œ
        xml= ET.parse(xml_path).getroot()

        # í™”ìƒ ë‚´ ë¬¼ì²´(object) ìˆ˜ ë§Œí¼ ë°˜ë³µ
        for obj in xml.iter('object'):

            # ì–´ë…¸í…Œì´ì…˜ì—ì„œ ê²€ì§€ê°€ difficultë¡œ ì„¤ì •ëœê²ƒì€ ì œì™¸
            difficult=int(obj.find('difficult').text)
            if difficult==1:
                continue

            # í•œ ë¬¼ì²´ì˜ ì–´ë…¸í…Œì´ì…˜ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
            bndbox=[]

            name=obj.find('name').text.lower().strip()
            bbox=obj.find('bndbox')

            # ì–´ë…¸í…Œì´ì…˜ì˜ xmin, ymin, xmax, ymaxë¥¼ ì·¨ë“í•˜ê³  0~1ë¡œ normalization
            pts=['xmin', 'ymin', 'xmax', 'ymax']

            for pt in (pts):
                # VOCëŠ” ì›ì ì„ (1,1)ë¡œ í•˜ê¸°ì— 1ì„ ë¹¼ì„œ ì›ì ì„ (0,0)ìœ¼ë¡œ ì¡°ì •í•œë‹¤
                cur_pixel=int(bbox.find(pt).text)-1

                # í­, ë†’ì´ë¡œ normalization
                if pt=='xmin' or pt=='xmax':
                    cur_pixel/=width
                else:
                    cur_pixel/=height
                bndbox.append(cur_pixel)

            # ì–´ë…¸í…Œì´ì…˜ì˜ í´ë˜ìŠ¤ëª… indexë¥¼ ì·¨ë“í•˜ì—¬ ì¶”ê°€

            label_idx=self.classes.index(name)
            bndbox.append(label_idx)

            ret+=[bndbox] # retì— xmin, ymin, xamx, ymax, label_indë¥¼ ë”í•œë‹¤
        return np.array(ret) #
```
ì¶œë ¥ì€ `xmin,ymin,xmax,ymax,label_ind`í˜•íƒœì´ë‹¤
ë™ì‘ì´ ì˜ ë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì
```
# ë™ì‘ í™•ì¸
voc_classes=['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']
transform=Anno_xml2list(voc_classes)

# í™”ìƒ ë¡œë“œìš©ìœ¼ë¡œ OpenCvì‚¬ìš©
ind=1
image_file_path=val_img_list[ind]
img=cv2.imread(image_file_path) # í™”ìƒì„ ì½ì–´ ë“¤ì„
height, width, channels=img.shape

# ì–´ë…¸í…Œì´ì…˜ì„ ë¦¬ìŠ¤íŠ¸ë¡œ í‘œì‹œ
transform_anno=transform(val_anno_list[ind], width, height)
print(transform_anno)
# [xmin, ymin, xmax, ymax, label_ind]
# 18 = train
# 14 = person
```
>[[ 0.09        0.03003003  0.998       0.996997   18.        ]
 [ 0.122       0.56756757  0.164       0.72672673 14.        ]]
 
ì˜ ì¶œë ¥ëœë‹¤. trainê³¼ personê°ì²´ê°€ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìœ¼ë©° BBoxì¢Œí‘œë„ ì•Œ ìˆ˜ ìˆë‹¤

#### ğŸ’¬ 3.imageì™€ annotation dataë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ ì‘ì„±
**imageì— ë³€í˜•ì´ ì¼ì–´ë‚˜ë©´ BBOXë„ ê°™ì´ ë³€í˜•ì‹œì¼œì¤˜ì•¼í•œë‹¤**
ì €ìì˜ ê¹ƒí—™ì— ìˆëŠ” .py íŒŒì¼ì—ì„œ augmentation í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•œë‹¤
```
# utils í´ë”ì— ìˆëŠ” data_augmentation.py ê°€ì ¸ì™€ì„œ ì‚¬ìš©
# ì…ë ¥ ì˜ìƒì˜ ì „ì²˜ë¦¬ class ì‘ì„±
from utils.data_augumentation import Compose, ConvertFromInts,ToAbsoluteCoords,PhotometricDistort,Expand,RandomSampleCrop,RandomMirror,ToPercentCoords,Resize,SubtractMeans

class DataTransform():
    """
    í™”ìƒê³¼ ì–´ë…¸í…Œì´ì…˜ì˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤, í›ˆë ¨ê³¼ ì¶”ë¡ ì—ì„œ ë‹¤ë¥´ê²Œ ì‘ì„±
    í™”ìƒ í¬ê¸°ëŠ” 300x300
    augmentation ìˆ˜í–‰

    Attributes
    ----------
    input_size: int
    color_mean: (B,G,R)
    """
    def __init__(self, input_size, color_mean):
        self.data_transform={
            'train':Compose([ConvertFromInts(), ToAbsoluteCoords(),PhotometricDistort(),Expand(color_mean),RandomSampleCrop(),RandomMirror(),ToPercentCoords(),Resize(input_size),SubtractMeans(color_mean)]),
            'val':Compose([ConvertFromInts(), ToAbsoluteCoords(),Resize(input_size),SubtractMeans(color_mean)])
        }

    def __call__(self,img,phase,boxes,labels):
        return self.data_transform[phase](img,boxes,labels)
```

#### ğŸ’¬ 4.DataSet ì‘ì„±í•˜ê¸°
- ì „ì²˜ë¦¬í•œ imageì˜ tensorí˜•ì‹ dataì™€ annotationì„ ì–»ì–´ë‚¸ë‹¤
```
# VOC2012 Dataset ì‘ì„±

class VOCDataset(data.Dataset):
    """
    VOC2012ì˜ Datasetì„ ë§Œë“œëŠ” í´ë˜ìŠ¤

    Attributes
    -----
    img_list
    anno_list
    phase
    transform
    transform_anno
    """
    def __init__(self,img_list,anno_list,phase,transform,transform_anno):
        self.img_list=img_list
        self.anno_list=anno_list
        self.phase=phase
        self.transform=transform
        self.transform_anno=transform_anno

    def __len__(self):
        return len(self.img_list) # í™”ìƒ ë§¤ìˆ˜ ë°˜í™˜
    
    def __getitem__(self,index):
        im,gt,he,w=self.pull_item(index) #ì „ì²˜ë¦¬í•œ í™”ìƒì˜ í…ì„œ í˜•ì‹ ë°ì´í„°ì™€ ì–´ë…¸í…Œì´ì…˜ ì·¨ë“
        return im,gt
    
    def pull_item(self,index):
        '''ì „ì²˜ë¦¬í•œ í™”ìƒì˜ í…ì„œ í˜•ì‹ ë°ì´í„°, ì–´ë…¸í…Œì´ì…˜, ë†’ì´, í­ ì·¨ë“'''

        #1.ì´ë¯¸ì§€ ì½ê¸°
        image_file_path=self.img_list[index]
        img=cv2.imread(image_file_path) # í™”ìƒì„ ì½ì–´ ë“¤ì„
        height,width,channels=img.shape

        #2. xmlí˜•ì‹ì˜ ì–´ë…¸í…Œì´ì…˜ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        anno_file_path=self.anno_list[index]
        anno_list=self.transform_anno(anno_file_path,width,height)

        #3.ì „ì²˜ë¦¬ ì‹¤ì‹œ
        img,boxes,labels=self.transform(img,self.phase,anno_list[:,:4],anno_list[:,4])

        #ìƒ‰ìƒ ì±„ë„ì˜ ìˆœì„œê°€ BGRì´ë¯€ë¡œ RGBë¡œ ë³€ê²½
        #ë†’ì´,í­,ì±„ë„->ì±„ë„,ë†’ì´,í­ ë³€ê²½
        img=torch.from_numpy(img[:,:,(2,1,0)]).permute(2,0,1)

        #BBOXì™€ ë¼ë²¨ì„ ì„¸íŠ¸ë¡œ í•œ np.arrayì‘ì„±,gt=ground truth
        gt=np.hstack((boxes,np.expand_dims(labels,axis=1)))

        return img,gt,height,width
```
ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ë³¸ë‹¤
```
#ë™ì‘ í™•ì¸
color_mean=(104,117,123)
input_size=300

train_dataset=VOCDataset(train_img_list,train_anno_list,phase="train",transform=DataTransform(input_size,color_mean),transform_anno=Anno_xml2list(voc_classes))
val_dataset=VOCDataset(val_img_list,val_anno_list,phase="val",transform=DataTransform(input_size,color_mean),transform_anno=Anno_xml2list(voc_classes))

#ì¶œë ¥ ì˜ˆ
val_dataset.__getitem__(1)
```
>(tensor([[[   0.9417,    6.1650,   11.1283,  ...,  -22.9083,  -13.2200,
             -9.4033],
          [   6.4367,    9.6600,   13.8283,  ...,  -21.4433,  -18.6500,
            -18.2033],
          [  10.8833,   13.5500,   16.7000,  ...,  -20.9917,  -24.5250,
            -25.1917],
          ...,
          [ -23.9500,  -14.9000,   -1.7583,  ..., -108.6083, -111.0000,
           -117.8083],
          [ -28.2817,  -20.1750,   -5.5633,  ..., -104.9933, -111.8350,
           -119.0000],
          [ -20.4767,  -21.0000,  -12.6333,  ..., -107.1683, -115.7800,
           -117.1100]],
 
         [[  25.9417,   30.1650,   35.1283,  ...,  -18.0767,  -14.7250,
            -11.8533],
          [  31.4367,   33.6600,   37.8283,  ...,  -13.5017,  -10.8250,
            -10.3783],
          [  35.7917,   37.5500,   40.7000,  ...,  -11.8417,  -13.0750,
            -14.0167],
          ...,
          [  -1.9500,    7.1000,   20.2417,  ..., -101.9083, -102.0000,
           -109.7167],
          [  -6.2817,    1.8250,   16.4367,  ..., -100.0517, -103.6700,
           -111.0000],
          [   1.5233,    1.0000,    9.3667,  ..., -102.5017, -107.7800,
           -109.1100]],
 
         [[  45.2750,   55.1650,   62.1283,  ...,   12.8500,   22.0550,
             27.8167],
          [  50.8800,   58.3300,   64.4983,  ...,   15.8350,   21.5150,
             22.7967],
          [  56.0667,   60.5500,   65.1500,  ...,   15.6417,   14.8250,
             14.7083],
          ...,
          [  36.7167,   43.1000,   56.2417,  ...,  -94.7583,  -96.0000,
           -101.9000],
          [  32.3850,   37.8250,   52.4367,  ...,  -92.1617,  -96.0000,
           -101.8867],
          [  40.1900,   37.0000,   45.3667,  ...,  -94.5017,  -99.7800,
            -99.1467]]]),
            array([[ 45.,  10., 499., 332.,  18.],[ 61., 189.,  82., 242.,  14.]]))
        
ì˜ ì¶œë ¥ì´ ëœë‹¤
### âœï¸ 2.3 DataLoader êµ¬í˜„
- dataë¥¼ mini batchë¡œ êº¼ë‚´ê¸° ìœ„í•œ DataLoader classë¥¼ êµ¬í˜„í•œë‹¤
- image dataë§ˆë‹¤ annotation ì •ë³´, gt(ground truth, imageë‚´ ë¬¼ì²´ ìˆ˜)ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì£¼ì˜í•´ì•¼í•œë‹¤.
-> collate_fn defë¥¼ ë³„ë„ë¡œ ë§Œë“¤ì–´ë†”ì•¼ í•œë‹¤.
#### ğŸ’¬ 1.collate_fn êµ¬í˜„
```
def od_collate_fn(batch):
    """
    Datasetì—ì„œ êº¼ë‚´ëŠ” ì–´ë…¸í…Œì´ì…˜ ë°ì´í„°ì˜ í¬ê¸°ëŠ” í™”ìƒë§ˆë‹¤ ë‹¤ë¥´ë‹¤.
    ex)í™”ìƒ ë‚´ ë¬¼ì²´ê°€ 2ê°œ ->(2,5), 3ê°œ ->(3,5)
    ë³€í™”ì— ëŒ€ì‘í•˜ëŠ” DataLoaderë¥¼ ë§Œë“¤ê¸° ìœ„í•´ collate_fnì„ ë§Œë“ ë‹¤.
    """
    targets=[]
    imgs=[]
    for sample in batch:
        imgs.append(sample[0]) #smaple0 = ì´ë¯¸ì§€
        targets.append(torch.FloatTensor(sample[1])) #sample1= ì–´ë…¸í…Œì´ì…˜ gt

    #imgsëŠ” ë¯¸ë‹ˆ ë°°ì¹˜ í¬ê¸°ì˜ ë¦¬ìŠ¤íŠ¸
    #ë¦¬ìŠ¤íŠ¸ ìš”ì†ŒëŠ” torch.Size([3,300,300])
    #torch.Size([batch_num,3,300,300])ìœ¼ë¡œ ë³€í™˜
    imgs=torch.stack(imgs,dim=0)

    #targetsì€ gt
    #ë¦¬ìŠ¤íŠ¸ì˜ í¬ê¸° = ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
    #targets ë¦¬ìŠ¤íŠ¸ ìš”ì†ŒëŠ [n,5]
    #nì€ í™”ìƒë§ˆë‹¤ ë‹¤ë¥´ë©° í™”ìƒ ì† ë¬¼ì²´ ìˆ˜
    #5ëŠ”[xmin,ymin,xmax,ymax,calss_index]

    return imgs,targets
```
ì˜ loadë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì
```
# í™•ì¸í•´ë³´ê¸°

batch_size=4
train_dataloader=data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=od_collate_fn)
val_dataloader=data.DataLoader(val_dataset,batch_size=batch_size,shuffle=False,collate_fn=od_collate_fn)

#ì‚¬ì „í˜• ë³€ìˆ˜ì— ì •ë¦¬
dataloaders_dict={"train":train_dataloader,"val":val_dataloader}

#ë™ì‘í™•ì¸
batch_iterator=iter(dataloaders_dict["val"])
images,targets=next(batch_iterator)
print(images.size())
print(len(targets))
print(targets[0].size())

#ë°°ì¹˜ 4ê°œ, 3ì±„ë„, 300x300ì´ë¯¸ì§€
#í™”ìƒ ë‚´ ë¬¼ì²´ 1ê°œ í™•ì¸
```
>torch.Size([4, 3, 300, 300])
4
torch.Size([1, 5])

ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ data ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì
```
print(train_dataset.__len__())
print(val_dataset.__len__())
```
>5717
5823

### âœï¸ 2.4 ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ êµ¬í˜„
< model >
- vgg
- extras
- loc
- conf

< sources >
- source1: vgg(conv4_3) -> L2Norm (512,38,38)(ì±„ë„,í¬ê¸°,í¬ê¸°)
- source2: vgg output (19x19)
- source3: source2 -> extras(conv1_2) , 10x10
- source4: source2 -> extras(conv2_2), 5x5
- source5: source2 -> extras(conv3_2), 3x3
- source6: source2 -> extras(conv4_2), 1x1

- source6ì€ ì»¤ë‹¤ë€ í•˜ë‚˜ì˜ ë¬¼ì²´ë¥¼ ê°ì§€
- source1ì€ ì‘ì€ ë¬¼ì²´ ê°ì§€
-> source6ì´ ë” ë§ì€ convë¥¼ ê±°ì³¤ìœ¼ë¯€ë¡œ image ì† ì‘ì€ ë¬¼ì²´ë¥¼ ê°ì§€í•˜ëŠ” ì •ë°€ë„ê°€ í° ë¬¼ì²´ì˜ ê°ì§€ë³´ë‹¤ ì‘ë‹¤

ì´ í›„ sourceë“¤ì„ loc networkì— ê±°ì¹˜ë©´ DBoxì˜ offeset ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³ 
conf networkì— ê±°ì¹˜ë©´ í´ë˜ìŠ¤ì˜ ì‹ ë¢°ë„ë¥¼ ì¶”ì¶œí•œë‹¤.


#### ğŸ’¬ VGG ëª¨ë“ˆ êµ¬í˜„
```
from math import sqrt
from itertools import product

import pandas as pd
import torch
from torch.autograd import Function
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
# VGGëª¨ë¸ êµ¬í˜„
def make_vgg():
    layers = []
    in_channels = 3  

    # ë°˜ë³µë¬¸ìœ¼ë¡œ VGGêµ¬í˜„
    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256,
           256, 'MC', 512, 512, 512, 'M', 512, 512, 512]

    for v in cfg:
        if v == 'M':
            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
        elif v == 'MC':
            # ceilì€ ì†Œìˆ˜ì ì„ ì˜¬ë ¤ ì •ìˆ˜ë¡œ
            # ë””í´íŠ¸ëŠ” ì†Œìˆ˜ì  ë²„ë ¤ ì •ìˆ˜ë¡œ
            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]
        else:
            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
            layers += [conv2d, nn.ReLU(inplace=True)]
            in_channels = v

    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)
    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)
    layers += [pool5, conv6,
               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]
    return nn.ModuleList(layers)


# í™•ì¸
vgg_test = make_vgg()
print(vgg_test)

```
#### ğŸ’¬ extra ëª¨ë“ˆ êµ¬í˜„
```
# extraëª¨ë¸ êµ¬í˜„
def make_extras():
    layers = []
    in_channels = 1024  # vgg outputì„ inputìœ¼ë¡œ ë°›ì„ ë•Œ ì±„ë„ ìˆ˜

    cfg = [256, 512, 128, 256, 128, 256, 128, 256]

    layers += [nn.Conv2d(in_channels, cfg[0], kernel_size=(1))]
    layers += [nn.Conv2d(cfg[0], cfg[1], kernel_size=(3), stride=2, padding=1)]
    layers += [nn.Conv2d(cfg[1], cfg[2], kernel_size=(1))]
    layers += [nn.Conv2d(cfg[2], cfg[3], kernel_size=(3), stride=2, padding=1)]
    layers += [nn.Conv2d(cfg[3], cfg[4], kernel_size=(1))]
    layers += [nn.Conv2d(cfg[4], cfg[5], kernel_size=(3))]
    layers += [nn.Conv2d(cfg[5], cfg[6], kernel_size=(1))]
    layers += [nn.Conv2d(cfg[6], cfg[7], kernel_size=(3))]
    
    #RELUë¥¼ ìˆœì „íŒŒì—ì„œ ì¤€ë¹„í•˜ê³  ëª¨ë“ˆì•ˆì—ì„œ x

    return nn.ModuleList(layers)



extras_test = make_extras()
print(extras_test)
```

#### ğŸ’¬ locë° conf ëª¨ë“ˆ êµ¬í˜„
```
# loc_layers=ë””í´íŠ¸ ë°•ìŠ¤ì˜ ì˜¤í”„ì…‹ ì¶œë ¥
# conf_layer=ë””í´íŠ¸ ë°•ìŠ¤ì˜ í´ë˜ìŠ¤ ì‹ ë¢°ë„ ì¶œë ¥


def make_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):

    loc_layers = []
    conf_layers = []

    # VGG 22ì¸µ, conv4_3(source1)ì˜ í•©ì„±ê³± ì¸µ
    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]
                             * 4, kernel_size=3, padding=1)]
    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]
                              * num_classes, kernel_size=3, padding=1)]

    # VGG ìµœì¢…ì¸µ, (source2)ì˜ í•©ì„±ê³± ì¸µ
    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]
                             * 4, kernel_size=3, padding=1)]
    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]
                              * num_classes, kernel_size=3, padding=1)]

    # extra(source3)ì˜ í•©ì„±ê³± ì¸µ
    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]
                             * 4, kernel_size=3, padding=1)]
    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]
                              * num_classes, kernel_size=3, padding=1)]

    # extra(source4)ì˜ í•©ì„±ê³± ì¸µ
    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]
                             * 4, kernel_size=3, padding=1)]
    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]
                              * num_classes, kernel_size=3, padding=1)]

    #extra(source5)ì˜ í•©ì„±ê³± ì¸µ
    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]
                             * 4, kernel_size=3, padding=1)]
    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]
                              * num_classes, kernel_size=3, padding=1)]

    # extra(source6)ì˜ í•©ì„±ê³± ì¸µ
    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]
                             * 4, kernel_size=3, padding=1)]
    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]
                              * num_classes, kernel_size=3, padding=1)]

    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)



loc_test, conf_test = make_loc_conf()
print(loc_test)
print(conf_test)
```
#### ğŸ’¬ L2Norm êµ¬í˜„
```
#L2 Norm ì¸µ êµ¬í˜„
class L2Norm(nn.Module):
    def __init__(self, input_channels=512, scale=20):
        super(L2Norm, self).__init__() #ë¶€ëª¨í´ë˜ìŠ¤ì˜ ìƒì„±ì ì‹¤í–‰
        self.weight = nn.Parameter(torch.Tensor(input_channels))
        self.scale = scale  # weight ì´ˆê¹ƒê°’
        self.reset_parameters()  # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        self.eps = 1e-10

    def reset_parameters(self):
        '''ê²°í•©íŒŒë¼ë¯¸í„°ì˜ scale í¬ê¸° ê°’ìœ¼ë¡œ ì´ˆê¸°í™” ì‹¤í–‰'''
        init.constant_(self.weight, self.scale)  #weightê°’ì´ ëª¨ë‘ scale(=20)ì´ ëœë‹¤

    def forward(self, x):
        '''38x38ì˜ íŠ¹ì§•ëŸ‰ì— ëŒ€í•´ 512 ì±„ë„ì— ê±¸ì³ ì œê³±í•©ì˜ ë£¨íŠ¸ êµ¬í–ˆë‹¤.
        38x38ê°œì˜ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì§•ëŸ‰ì„ ì •ê·œí™”í•œ í›„ ê³„ìˆ˜ë¥¼ ê³±í•˜ì—¬ ê³„ì‚°í•˜ëŠ” ì¸µ'''

   
        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps
        x = torch.div(x, norm)

        weights = self.weight.unsqueeze(
            0).unsqueeze(2).unsqueeze(3).expand_as(x)
        out = weights * x

        return out
```


#### ğŸ’¬ DBOX êµ¬í˜„
- source1,5,6ì—ëŠ” 4ê°œì˜ DBOX, 2,3,4ì—ëŠ” 6ê°œì˜ DBOX ì¤€ë¹„
- ê°ê°ì˜ DBOX í¬ê¸°ë„ ë‹¤ì–‘í•˜ê²Œ êµ¬í˜„
```
# ë””í´íŠ¸ ë°•ìŠ¤ ì¶œë ¥í•˜ëŠ” í´ë˜ìŠ¤
class DBox(object):
    def __init__(self, cfg):
        super(DBox, self).__init__()

        # ì´ˆê¸°ì„¤ì •
        self.image_size = cfg['input_size']  # ì´ë¯¸ì§€ í¬ê¸° 300
        # [38, 19, â€¦] ê° sourceì˜ feature mapì˜ í¬ê¸°
        self.feature_maps = cfg['feature_maps']
        self.num_priors = len(cfg["feature_maps"])  # source = 6ê°œ
        self.steps = cfg['steps']  # [8, 16, â€¦] DBoxì˜ pixel í¬ê¸°
        
        self.min_sizes = cfg['min_sizes']
        # [30, 60, â€¦] ì‘ì€ ì •ì‚¬ê°í˜•ì˜ DBOX ë©´ì 
        
        self.max_sizes = cfg['max_sizes']
        # [60, 111, â€¦] í° ì •ì‚¬ê°í˜•ì˜ DBOX ë©´ì 
        
        self.aspect_ratios = cfg['aspect_ratios']  # DBOXì˜ ì¢…íš¡ë¹„

    def make_dbox_list(self):
        '''DBoxã‚’ä½œæˆã™ã‚‹'''
        mean = []
        # 'feature_maps': [38, 19, 10, 5, 3, 1]
        for k, f in enumerate(self.feature_maps):
            for i, j in product(range(f), repeat=2):  
                # feature map í™”ìƒ í¬ê¸°
                # 300 / 'steps': [8, 16, 32, 64, 100, 300],
                f_k = self.image_size / self.steps[k]

                # ì •ê·œí™”
                cx = (j + 0.5) / f_k
                cy = (i + 0.5) / f_k

                # í™”ë©´ë¹„ 1ì˜ ì‘ìœ¼ DBOX [cx,cy, width, height]
                # 'min_sizes': [30, 60, 111, 162, 213, 264]
                s_k = self.min_sizes[k]/self.image_size
                mean += [cx, cy, s_k, s_k]

                # í™”ë©´ë¹„ 1ì˜ í° DBox [cx,cy, width, height]
                # 'max_sizes': [60, 111, 162, 213, 264, 315],
                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))
                mean += [cx, cy, s_k_prime, s_k_prime]

                # ê·¸ì™¸ í™”ë©´ë¹„ì˜ defBox [cx,cy, width, height]
                for ar in self.aspect_ratios[k]:
                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]
                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]

        # DBoxë¥¼ í…ì„œë¡œ ì „í™˜
        output = torch.Tensor(mean).view(-1, 4)

        # DBOXí¬ê¸° ì¡°ì •
        output.clamp_(max=1, min=0)

        return output

```
DBOXì •ë³´ê°€ ì˜ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•´ë³´ì
```
# ë™ì‘ í™•ì¸
# SSD300 ì„¤ì •
ssd_cfg = {
    'num_classes': 21,  
    'input_size': 300,
    'bbox_aspect_num': [4, 6, 6, 6, 4, 4], 
    'feature_maps': [38, 19, 10, 5, 3, 1],  
    'steps': [8, 16, 32, 64, 100, 300], 
    'min_sizes': [30, 60, 111, 162, 213, 264],  
    'max_sizes': [60, 111, 162, 213, 264, 315], 
    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],
}

# DBox
dbox = DBox(ssd_cfg)
dbox_list = dbox.make_dbox_list()

# DBox ì¶œë ¥í™•ì¸
pd.DataFrame(dbox_list.numpy())
```
>
0	1	2	3
0	0.013333	0.013333	0.100000	0.100000
1	0.013333	0.013333	0.141421	0.141421
2	0.013333	0.013333	0.141421	0.070711
3	0.013333	0.013333	0.070711	0.141421
4	0.040000	0.013333	0.100000	0.100000
...	...	...	...	...
8727	0.833333	0.833333	0.502046	1.000000
8728	0.500000	0.500000	0.880000	0.880000
8729	0.500000	0.500000	0.961249	0.961249
8730	0.500000	0.500000	1.000000	0.622254
8731	0.500000	0.500000	0.622254	1.000000
8732 rows Ã— 4 columns

#### ğŸ’¬ SSD class êµ¬í˜„
ìœ„ì— ëª¨ë“ˆë“¤ì„ ë‹¤ ì„¸íŒ…í•´ë†¨ìœ¼ë¯€ë¡œ SSD classë¥¼ êµ¬í˜„í•´ë³´ì
```
# SSD í´ë˜ìŠ¤ ì‘ì„±
class SSD(nn.Module):

    def __init__(self, phase, cfg):
        super(SSD, self).__init__()

        self.phase = phase  # train or inference
        self.num_classes = cfg["num_classes"]  # í´ë˜ìŠ¤ ìˆ˜ =21

        # SSD ë„¤íŠ¸ì›Œí¬ ì‘ì„±
        self.vgg = make_vgg()
        self.extras = make_extras()
        self.L2Norm = L2Norm()
        self.loc, self.conf = make_loc_conf(
            cfg["num_classes"], cfg["bbox_aspect_num"])

        # DBoxì‘ì„±
        dbox = DBox(cfg)
        self.dbox_list = dbox.make_dbox_list()

        # inferenceì‹œ Detect()
        if phase == 'inference':
            self.detect = Detect()


# ë™ì‘í™•ì¸
ssd_test = SSD(phase="train", cfg=ssd_cfg)
print(ssd_test)
```
>SSD(
  (vgg): ModuleList(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))
    (32): ReLU(inplace=True)
    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    (34): ReLU(inplace=True)
  )
  (extras): ModuleList(
    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
  )
  (L2Norm): L2Norm()
  (loc): ModuleList(
    (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4-5): 2 x Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (conf): ModuleList(
    (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4-5): 2 x Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)

ê°ê°ì˜ ëª¨ë“ˆë“¤ì´ ì˜ êµ¬í˜„ëë‹¤.
ë‹¤ìŒì£¼ì— ìˆœì „íŒŒ í•¨ìˆ˜ë¶€í„° ì´ì–´ì„œ êµ¬í˜„í•´ë³´ê² ë‹¤.
